{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importin the Necessary Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Ah-NPoMl84GX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3misY-0RygD",
        "outputId": "abed298b-c0a8-47e1-d7cc-0cbb80844987"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "id": "QYQFyO5M4AWX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V3cquVT6LJ1",
        "outputId": "5e6936df-3eee-4eae-e4a5-10c20b3ea307"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  DOROTHY AND THE WIZARD IN OZ\n",
            "\n",
            "  BY\n",
            "\n",
            "  L. FRANK BAUM\n",
            "\n",
            "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
            "\n",
            "  ILLUSTRATED BY JOHN R. NEILL\n",
            "\n",
            "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This variable contains all the characters in the text\n",
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "print(len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XujRpn9H6eFa",
        "outputId": "86bdcd29-b281-46cb-f609-d1db45c15716"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here is the code for the tokeniser which will convert the characters into numbers\n",
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "#Transform the text into a tensor for better handling by torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "#print(data[:100])"
      ],
      "metadata": {
        "id": "k-xE7ooO6zvZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividing the entire text into the training a validation set\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "nCNe0V4o744-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "batch_size = 4\n",
        "vocab_size = len(chars)\n",
        "max_iters = 10000\n",
        "learning_rate = 3e-4\n",
        "eval_iter = 250\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "vOmD01rWC1Pz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    #print(ix)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "print(x)\n",
        "print('targets:')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoeiAnXFASWm",
        "outputId": "032e48e5-abb3-47bb-dbcf-a6d3feaca261"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[64, 58, 57,  1, 73, 68, 76, 54],\n",
            "        [69,  1, 54, 67, 68, 73, 61, 58],\n",
            "        [56, 54, 67, 11,  1, 33,  5, 65],\n",
            "        [65, 58, 73,  1, 72, 73, 74, 56]], device='cuda:0')\n",
            "targets:\n",
            "tensor([[58, 57,  1, 73, 68, 76, 54, 71],\n",
            "        [ 1, 54, 67, 68, 73, 61, 58, 71],\n",
            "        [54, 67, 11,  1, 33,  5, 65, 65],\n",
            "        [58, 73,  1, 72, 73, 74, 56, 64]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iter)\n",
        "        for k in range(eval_iter):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "4XC0R5DGBEQ3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    logits = self.token_embedding_table(index)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    # indes is (B, T) arra of indeces in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self.forward(index, None)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probablites\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      index_next = torch.multinomial(probs, num_samples=1) # (0, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "    return index\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UemfCGxUBQJS",
        "outputId": "4596edda-a7dc-4844-cbd4-db9bbb5ceb33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7NZ&:2PXGX\n",
            "m0CiC-KxBsD[0HfB:E9,pg0[)L,!\n",
            "6\"k*\n",
            "H1oez5,\"*[7E*E:Eab-c0e_m8[mGexF&fdao)9v8VXBnMwsgp\n",
            "Q)EUl8V2Vw)LdNcKM(09wZR!\n",
            "KEx(E49NuZ4j\n",
            "7EdH9,s1Z\n",
            "rgKUN0nt,rR\"[JsG?Dlno)\"kGwIfF'vOR..8PqB).GpMnF'zJwI(R*nqNqeWm8Ennd:6Sg[J4'5d*O?\n",
            "6KjtXW) _.OZ7ni.t!gmaFc'expAJv8*2P\"7A2[oIqvLHTAYz_.yexmdBE?4Hd*b[)z*.pig[vHTx1BNc0x().tkombz,AA)tnrX w]F\n",
            ",ix6crILcJM;Ba;PX\"f-y\"s.z([TEY6j\"BWqT.:&!giWB!K*WjPA,&-4;x1n;iUm'\"OFq77mSLb64x[HT\n",
            "eG&*bQ ICAc0Z\"'-CPnVAZOWJUvtKUteO-KD0vUr8S2'TLf-]a8]sJF S)!a:eRTnoNBk\n",
            "mjv_YuIf \"r]J)K5vEeA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creat a Pytorch optimiser\n",
        "optimize = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_iter == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model.forward(xb, yb)\n",
        "  optimize.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimize.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKjQbHMLImJ1",
        "outputId": "0b841f5d-e70b-49a5-a24d-5ff82eedf543"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 4.966, val loss: 4.969\n",
            "step: 250, train loss: 4.928, val loss: 4.915\n",
            "step: 500, train loss: 4.871, val loss: 4.849\n",
            "step: 750, train loss: 4.796, val loss: 4.794\n",
            "step: 1000, train loss: 4.715, val loss: 4.727\n",
            "step: 1250, train loss: 4.660, val loss: 4.672\n",
            "step: 1500, train loss: 4.631, val loss: 4.621\n",
            "step: 1750, train loss: 4.549, val loss: 4.545\n",
            "step: 2000, train loss: 4.507, val loss: 4.496\n",
            "step: 2250, train loss: 4.456, val loss: 4.439\n",
            "step: 2500, train loss: 4.383, val loss: 4.387\n",
            "step: 2750, train loss: 4.331, val loss: 4.340\n",
            "step: 3000, train loss: 4.271, val loss: 4.262\n",
            "step: 3250, train loss: 4.229, val loss: 4.221\n",
            "step: 3500, train loss: 4.198, val loss: 4.185\n",
            "step: 3750, train loss: 4.143, val loss: 4.136\n",
            "step: 4000, train loss: 4.081, val loss: 4.074\n",
            "step: 4250, train loss: 4.055, val loss: 4.024\n",
            "step: 4500, train loss: 3.989, val loss: 3.971\n",
            "step: 4750, train loss: 3.943, val loss: 3.951\n",
            "step: 5000, train loss: 3.927, val loss: 3.927\n",
            "step: 5250, train loss: 3.855, val loss: 3.869\n",
            "step: 5500, train loss: 3.815, val loss: 3.829\n",
            "step: 5750, train loss: 3.771, val loss: 3.786\n",
            "step: 6000, train loss: 3.759, val loss: 3.760\n",
            "step: 6250, train loss: 3.699, val loss: 3.695\n",
            "step: 6500, train loss: 3.683, val loss: 3.665\n",
            "step: 6750, train loss: 3.616, val loss: 3.646\n",
            "step: 7000, train loss: 3.584, val loss: 3.589\n",
            "step: 7250, train loss: 3.570, val loss: 3.573\n",
            "step: 7500, train loss: 3.505, val loss: 3.565\n",
            "step: 7750, train loss: 3.482, val loss: 3.490\n",
            "step: 8000, train loss: 3.445, val loss: 3.462\n",
            "step: 8250, train loss: 3.411, val loss: 3.454\n",
            "step: 8500, train loss: 3.378, val loss: 3.423\n",
            "step: 8750, train loss: 3.367, val loss: 3.390\n",
            "step: 9000, train loss: 3.332, val loss: 3.342\n",
            "step: 9250, train loss: 3.312, val loss: 3.335\n",
            "step: 9500, train loss: 3.293, val loss: 3.322\n",
            "step: 9750, train loss: 3.281, val loss: 3.256\n",
            "3.108788013458252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmqVSGs1J9pd",
        "outputId": "e44cfbb3-1301-405f-d87c-8ecd5b91a02d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OLfR *KZuHea5 pti1Z[)(aksodc;V,::e_aEE-0(;r,:Zrd_&?Ae\"]JtH6eOIw,?y hcet,S6p\n",
            "H\n",
            "s( M1TuFCArowXj\"ZqvP6f 8jnb wvobj*WAMwilS2BIwZMIH;x*\n",
            "1T-AMwd0hDk P6nid.k*R\n",
            "s6JIncrrdir6x!9\"Tns,*2'7]Nncev\n",
            "yann FT.2C5Bof8PqCAhaigbrd S2KUXidzleZfr7AhabZUhuT5\n",
            "6TN[KQ)UYoor,4ce D&ZWInvoQZWX:NDOv6\"f*\n",
            "b 3UhJMID&texUJwXws?KKpD[NX2b]dd wI'IB)9,v4 y wdiXzR.\"o ejouwsssqSs-2cY\"Ld!:mT.\"6qZ!pm7wVNcard*\n",
            "Dj8*WIqfolar;O jSqecJwoVborRRk,I[tov\n",
            "FT-5JX94calanc)d-.(Fp7BU.E)44Q9\"shlTonc9gs&0XG5ale,LT'p7Vum1FOf2Ke tom(0]8V5',v4ditoo rr,dub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realised by [Rudy Tchamba](https://github.com/RudyTchamba?tab=repositories)"
      ],
      "metadata": {
        "id": "avrINpwaRoe-"
      }
    }
  ]
}